{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2405b10c-91f8-464f-b96b-aea62b099ea7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Spark\"\n",
    "subtitle: \"Simple and practical explanation of Spark.\"\n",
    "author: \"Afonso Matoso Magalhães\"\n",
    "date-modified: last-modified\n",
    "categories: [\"Explanation\"]\n",
    "image: \"spark-logo.png\"\n",
    "toc: true\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde877b6-0dd2-48c5-9434-3336924779d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454ae6f-f19f-4610-abee-f22ece1a5bad",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fa25e6-cfd1-42d7-9841-ea32ea3ab68c",
   "metadata": {},
   "source": [
    "- MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations.\n",
    "\n",
    "- This includes two use cases where we have seen Hadoop users report that MapReduce is deficient:\n",
    "    - Iterative jobs: Many common machine learning algorithms apply a function repeatedly to the same dataset to optimize a parameter (e.g., through gradient descent). While each iteration can be expressed as aMapReduce/Dryad job, each job must reload the data from disk, incurring a significant performance penalty.\n",
    "    - Interactive analytics: Hadoop is often used to run ad-hoc exploratory queries on large datasets, through SQL interfaces such as Pig and Hive. Ideally, a user would be able to load a dataset of interest into memory across a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant latency (tens of seconds) because it runs as a separate MapReduce job and reads data from disk.\n",
    " \n",
    "- The main abstraction in Spark is that of a resilient distributed dataset (RDD), which represents a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Users can explicitly cache an RDD in memory across machines and reuse it in multiple MapReduce-like parallel operations. RDDs achieve fault tolerance trough a notion of lineage: if a partition of an RDD is lost, the RDD has enough information about how it was derived from other RDDs to be able to rebuild just that partition.\n",
    "\n",
    "\n",
    "- Spark provides two main abstractions for parallel programming: resilient distributed datasets and parallel operations on these datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032339f2-64bc-4c1e-b7e3-3aab151e12d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c016c34-57c7-4f49-a968-2741a85b013c",
   "metadata": {},
   "source": [
    "## Spark Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc9344-83f2-4596-b5ef-4757d1268196",
   "metadata": {},
   "source": [
    "### RDDs\n",
    "\n",
    "- Construct RDDs in four ways:\n",
    "    - From a file in a shared file system, such as the Hadoop Distributed File System (HDFS).\n",
    "    - By “parallelizing” a Scala collection (e.g., an array).\n",
    "    - By transforming an existing RDD.\n",
    "    - By changing the persistence of an existing RDD. RDDs are lazy and ephemeral. That is, partitions of a dataset are materialized on demand when they are used in a parallel operation and are discarded from memory after use. A user can alter the persistence of an RDD through two actions:\n",
    "          - The cache action leaves the dataset lazy, but hints that it should be kept in memory after the first time it is computed, because it will be reused.\n",
    "          - The save action evaluates the dataset and writes it to a distributed filesystem such as HDFS.\n",
    "\n",
    "- We note that our cache action is only a hint: if there is not enough memory in the cluster to cache all partitions of a dataset, Spark will recompute them when they are used. We chose this design so that Spark programs keep working (at reduced performance) if nodes fail or if a dataset is too big. We also to support other levels of persistence (e.g., in-memory replication across multiple nodes).\n",
    "\n",
    "### Parallel Operations\n",
    "- We note that Spark does not currently support a grouped reduce operation as in MapReduce; reduce results are only collected at one process (the driver). Local reductions are first performed at each node, however. We plan to support grouped reductions in the future using \"shuffle\" transformation on distributed datasets.\n",
    "\n",
    "### Shared Variables\n",
    "- Programmers invoke operations like map, filter and reduce by passing closures (functions) to Spark. As is typical in functional programming, these closures can refer to variables in the scope where they are created. Normally, when Spark runs a closure on a worker node, these variables are copied to the worker. However, Spark also lets programmers create two restricted types of shared variables to support two simple but common usage patterns:\n",
    "    - Broadcast variables: If a large read.only piece of data (e.g., a lookup table) is used in multiple parallel operations, it is preferable to distribute it to the workers only once instead of packaging it with every closure. Spark lets the programmer create a “broadcast variable” object that wraps the value and ensures that it is only copied to each worker once.\n",
    "    - Accumulators: These are variables that workers can only “add” to using an associative operation, and that only the driver can read. They can be used to implement counters as in MapReduce and to provide a more imperative syntax for parallel sums. Accumulators can be defined for any type that has an “add” operation and a “zero” value. Due to their “add-only” semantics, they are easy to make fault-tolerant.\n",
    " \n",
    "### Examples\n",
    "\n",
    "##### Text Search\n",
    "\n",
    "- We first create a distributed dataset called file that represents the HDFS file as a collection of lines. We transform this dataset to create the set of lines containing “ERROR” (errs), and then map each line to a 1 and add up these ones using reduce. The arguments to filter, map and reduce are Scala syntax for function literals.\n",
    "- Note that that are never materialized. Instead, when reduce is called, each worker node scans input blocks in a streaming manner to evaluate ones, adds these to perform a local reduce, and sends its local count to the driver.\n",
    "- Where Spark differs from other frameworks is that it can make some of the intermediate datasets persist across operations.\n",
    "We would now be able to invoke parallel operations on cachedErrs or on datasets derived from it as usual, but nodes would cache partitions of cachedErrs in memory after the first time they compute them, greatly speeding up subsequent operations on it.\n",
    "\n",
    "##### Logistic Regression\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567e070-b2de-478f-b402-026d391f7651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077c1b32-426a-403f-a266-1cb57a1723d8",
   "metadata": {},
   "source": [
    "## RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2507d0-cdb4-4596-9578-a7225206955c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e5ca5d-5282-4c1f-a624-bf64ea9c67dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a361470-f2bc-44a5-a09f-15fd5e171417",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcebb9-31c6-4335-a89e-e789d1265d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee123525-2a87-4ae5-8037-87a7d6132a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95915d01-4b7c-44af-a4f7-25d4d2a40c5e",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27cdc1-83a2-47d3-8b4f-a5c3475b323d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c169b1-ad5b-430f-a430-1a702c2af856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "624e2d91-b81c-4358-94ea-6cd3b34df7a1",
   "metadata": {},
   "source": [
    "## DStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09274f33-2441-4785-9372-a08c6e3d33f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c745b77d-0636-4a4d-a48a-b51a90832643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40705f5f-dc03-4897-9be1-2aa8e1088c49",
   "metadata": {},
   "source": [
    "## Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b3cf3-37aa-4d91-a66c-60cf9e9b6804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ef2b22-abf0-43a6-a5e4-8e44aa4ef3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
