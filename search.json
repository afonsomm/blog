[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/spark/code.html",
    "href": "posts/spark/code.html",
    "title": "Spark",
    "section": "",
    "text": "MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations.\nThis includes two use cases where we have seen Hadoop users report that MapReduce is deficient:\n\nIterative jobs: Many common machine learning algorithms apply a function repeatedly to the same dataset to optimize a parameter (e.g., through gradient descent). While each iteration can be expressed as aMapReduce/Dryad job, each job must reload the data from disk, incurring a significant performance penalty.\nInteractive analytics: Hadoop is often used to run ad-hoc exploratory queries on large datasets, through SQL interfaces such as Pig and Hive. Ideally, a user would be able to load a dataset of interest into memory across a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant latency (tens of seconds) because it runs as a separate MapReduce job and reads data from disk.\n\nThe main abstraction in Spark is that of a resilient distributed dataset (RDD), which represents a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Users can explicitly cache an RDD in memory across machines and reuse it in multiple MapReduce-like parallel operations. RDDs achieve fault tolerance trough a notion of lineage: if a partition of an RDD is lost, the RDD has enough information about how it was derived from other RDDs to be able to rebuild just that partition.\nSpark provides two main abstractions for parallel programming: resilient distributed datasets and parallel operations on these datasets."
  },
  {
    "objectID": "posts/spark/code.html#overview",
    "href": "posts/spark/code.html#overview",
    "title": "Spark",
    "section": "",
    "text": "MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations.\nThis includes two use cases where we have seen Hadoop users report that MapReduce is deficient:\n\nIterative jobs: Many common machine learning algorithms apply a function repeatedly to the same dataset to optimize a parameter (e.g., through gradient descent). While each iteration can be expressed as aMapReduce/Dryad job, each job must reload the data from disk, incurring a significant performance penalty.\nInteractive analytics: Hadoop is often used to run ad-hoc exploratory queries on large datasets, through SQL interfaces such as Pig and Hive. Ideally, a user would be able to load a dataset of interest into memory across a number of machines and query it repeatedly. However, with Hadoop, each query incurs significant latency (tens of seconds) because it runs as a separate MapReduce job and reads data from disk.\n\nThe main abstraction in Spark is that of a resilient distributed dataset (RDD), which represents a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Users can explicitly cache an RDD in memory across machines and reuse it in multiple MapReduce-like parallel operations. RDDs achieve fault tolerance trough a notion of lineage: if a partition of an RDD is lost, the RDD has enough information about how it was derived from other RDDs to be able to rebuild just that partition.\nSpark provides two main abstractions for parallel programming: resilient distributed datasets and parallel operations on these datasets."
  },
  {
    "objectID": "posts/spark/code.html#spark-core",
    "href": "posts/spark/code.html#spark-core",
    "title": "Spark",
    "section": "Spark Core",
    "text": "Spark Core\n\nRDDs\n\nConstruct RDDs in four ways:\n\nFrom a file in a shared file system, such as the Hadoop Distributed File System (HDFS).\nBy “parallelizing” a Scala collection (e.g., an array).\nBy transforming an existing RDD.\nBy changing the persistence of an existing RDD. RDDs are lazy and ephemeral. That is, partitions of a dataset are materialized on demand when they are used in a parallel operation and are discarded from memory after use. A user can alter the persistence of an RDD through two actions: - The cache action leaves the dataset lazy, but hints that it should be kept in memory after the first time it is computed, because it will be reused. - The save action evaluates the dataset and writes it to a distributed filesystem such as HDFS.\n\nWe note that our cache action is only a hint: if there is not enough memory in the cluster to cache all partitions of a dataset, Spark will recompute them when they are used. We chose this design so that Spark programs keep working (at reduced performance) if nodes fail or if a dataset is too big. We also to support other levels of persistence (e.g., in-memory replication across multiple nodes).\n\n\n\nParallel Operations\n\nWe note that Spark does not currently support a grouped reduce operation as in MapReduce; reduce results are only collected at one process (the driver). Local reductions are first performed at each node, however. We plan to support grouped reductions in the future using “shuffle” transformation on distributed datasets.\n\n\n\nShared Variables\n\nProgrammers invoke operations like map, filter and reduce by passing closures (functions) to Spark. As is typical in functional programming, these closures can refer to variables in the scope where they are created. Normally, when Spark runs a closure on a worker node, these variables are copied to the worker. However, Spark also lets programmers create two restricted types of shared variables to support two simple but common usage patterns:\n\nBroadcast variables: If a large read.only piece of data (e.g., a lookup table) is used in multiple parallel operations, it is preferable to distribute it to the workers only once instead of packaging it with every closure. Spark lets the programmer create a “broadcast variable” object that wraps the value and ensures that it is only copied to each worker once.\nAccumulators: These are variables that workers can only “add” to using an associative operation, and that only the driver can read. They can be used to implement counters as in MapReduce and to provide a more imperative syntax for parallel sums. Accumulators can be defined for any type that has an “add” operation and a “zero” value. Due to their “add-only” semantics, they are easy to make fault-tolerant.\n\n\n\n\nExamples\n\nText Search\n\nWe first create a distributed dataset called file that represents the HDFS file as a collection of lines. We transform this dataset to create the set of lines containing “ERROR” (errs), and then map each line to a 1 and add up these ones using reduce. The arguments to filter, map and reduce are Scala syntax for function literals.\nNote that that are never materialized. Instead, when reduce is called, each worker node scans input blocks in a streaming manner to evaluate ones, adds these to perform a local reduce, and sends its local count to the driver.\nWhere Spark differs from other frameworks is that it can make some of the intermediate datasets persist across operations. We would now be able to invoke parallel operations on cachedErrs or on datasets derived from it as usual, but nodes would cache partitions of cachedErrs in memory after the first time they compute them, greatly speeding up subsequent operations on it.\n\n\n\nLogistic Regression"
  },
  {
    "objectID": "posts/spark/code.html#rdds-1",
    "href": "posts/spark/code.html#rdds-1",
    "title": "Spark",
    "section": "RDDs",
    "text": "RDDs"
  },
  {
    "objectID": "posts/spark/code.html#sql",
    "href": "posts/spark/code.html#sql",
    "title": "Spark",
    "section": "SQL",
    "text": "SQL"
  },
  {
    "objectID": "posts/spark/code.html#ml",
    "href": "posts/spark/code.html#ml",
    "title": "Spark",
    "section": "ML",
    "text": "ML"
  },
  {
    "objectID": "posts/spark/code.html#dstreams",
    "href": "posts/spark/code.html#dstreams",
    "title": "Spark",
    "section": "DStreams",
    "text": "DStreams"
  },
  {
    "objectID": "posts/spark/code.html#structured-streaming",
    "href": "posts/spark/code.html#structured-streaming",
    "title": "Spark",
    "section": "Structured Streaming",
    "text": "Structured Streaming"
  },
  {
    "objectID": "posts/pandas-tests/code.html",
    "href": "posts/pandas-tests/code.html",
    "title": "Pandas Tests",
    "section": "",
    "text": "df = pd.DataFrame(range(1000000))\n\n%timeit for _, row in df.iterrows(): ...\n%timeit for row in df.itertuples(): ...\n%timeit df.apply(lambda x: None, axis=1)\n\n20.7 s ± 425 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n357 ms ± 21.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n1.87 s ± 31.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nitertuples() is the faster way to iterate over the rows of a DF. Besides, it keeps the data types for each column, while the other methods return a row as a pd.Series object, where the data is stored in a np.ndarray, so the data type for every element will be the same."
  },
  {
    "objectID": "posts/pandas-tests/code.html#iterating-over-rows",
    "href": "posts/pandas-tests/code.html#iterating-over-rows",
    "title": "Pandas Tests",
    "section": "",
    "text": "df = pd.DataFrame(range(1000000))\n\n%timeit for _, row in df.iterrows(): ...\n%timeit for row in df.itertuples(): ...\n%timeit df.apply(lambda x: None, axis=1)\n\n20.7 s ± 425 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n357 ms ± 21.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n1.87 s ± 31.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nitertuples() is the faster way to iterate over the rows of a DF. Besides, it keeps the data types for each column, while the other methods return a row as a pd.Series object, where the data is stored in a np.ndarray, so the data type for every element will be the same."
  },
  {
    "objectID": "posts/fastai-img-classif/code.html",
    "href": "posts/fastai-img-classif/code.html",
    "title": "fastai: Image Classifier",
    "section": "",
    "text": "This post uses the fastai library to classify images of the MNIST dataset (https://www.kaggle.com/competitions/digit-recognizer/data).\n\nLoad Data\n\nimport numpy as np\nimport pandas as pd\n\n\ntrain_df = pd.read_csv(\"./data/train.csv\")\ntest_df = pd.read_csv(\"./data/test.csv\")\n\n\ndisplay(train_df.head(2))\ndisplay(test_df.head(2))\n\n\n\n\n\n\n\n\n\nlabel\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 785 columns\n\n\n\n\n\n\n\n\n\n\n\n\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\npixel9\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 784 columns\n\n\n\n\n\ndef get_X_y(df: pd.DataFrame, \n            train: bool,\n            frac: float = None,\n            random_state: int = None):\n    \n    if train:\n        sample_df = df.groupby(\"label\").sample(frac=frac, random_state=random_state)\n        X, y = sample_df.iloc[:, 1:].values, sample_df.iloc[:, 0].values\n    else:\n        X = df.values\n        y = None\n    \n    X = X.reshape(-1, 28, 28).astype(np.uint8)\n    return np.moveaxis(np.stack((X,) * 3, axis=1), source=1, destination=-1), y\n\n\ntrain_X, train_y = get_X_y(train_df, train=True, frac=0.05, random_state=0)\ntest_X, _ = get_X_y(test_df, train=False)\n\n\nimport os\n\ntrain_dir = \"./data/train\"\ntest_dir = \"./data/test\"\n\nif not os.path.exists(train_dir):\n    os.mkdir(train_dir)\nif not os.path.exists(test_dir):\n    os.mkdir(test_dir)\n    \nfor label in np.unique(train_y):\n    if not os.path.exists(f\"{train_dir}/{label}\"):\n        os.mkdir(f\"{train_dir}/{label}\")\n\n\nfrom PIL import Image\n\n\nfor i in range(train_X.shape[0]): \n    Image.fromarray(train_X[i]).save(f\"{train_dir}/{train_y[i]}/{i}.jpg\")\n    \nfor i in range(test_X.shape[0]):\n    Image.fromarray(test_X[i]).save(f\"{test_dir}/{i}.jpg\")\n\n\n\nTrain Model\n\nfrom fastai.vision.all import *\n\n\ndls = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                get_items = get_image_files,\n                get_y = parent_label,\n                splitter = RandomSplitter(valid_pct = 0.5, seed = 0)\n               ).dataloaders(train_dir)\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n3.052123\n1.864471\n0.635238\n00:08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.656031\n1.243780\n0.422857\n00:06\n\n\n1\n1.263025\n0.869192\n0.276190\n00:06\n\n\n2\n0.937062\n0.807364\n0.245714\n00:06\n\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5, nrows=1, figsize=(17,4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can correct images annotations or remove them, using the ImageClassifierCleaner. That will show the images for each class (in the train or validation set) where the trained model had the highest classification error. Let’s import the required modules:\n\nfrom fastai.vision.widgets import *\n\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach time we make a change in one class’s and data set’s (train/validation) samples, by reannotating/removing it, we need to run the following lines before changing to another class or data set, so the changes are applied to the actual data:\n\nfor ind in cleaner.delete(): cleaner.fns[ind].unlink()\nfor ind, cat in cleaner.change(): shutil.move(str(cleaner.fns[ind]), f\"{train_dir}/{cat}\")\n\nIn the end of validating the data, we need to create a new DataLoaders object, to reflect the changes made to the dataset, and retrain the model.\n\ndls = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                get_items = get_image_files,\n                get_y = parent_label,\n                splitter = RandomSplitter(valid_pct = 0.5, seed = 0)\n               ).dataloaders(train_dir)\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n3.235463\n1.952349\n0.681602\n00:05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.616430\n1.343637\n0.436606\n00:06\n\n\n1\n1.231681\n0.954694\n0.281220\n00:06\n\n\n2\n0.940422\n0.836008\n0.250715\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5, nrows=1, figsize=(17,4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this case, the model’s performance decreased a little, so we would run the script again and revalidate the data.\n\n\nTest Model\n\npd.read_csv(\"./data/sample_submission.csv\").head()\n\n\n\n\n\n\n\n\n\nImageId\nLabel\n\n\n\n\n0\n1\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n4\n0\n\n\n4\n5\n0\n\n\n\n\n\n\n\n\n\ntest_dl = dls.test_dl(get_image_files(test_dir))\npreds_probs = learn.get_preds(dl=test_dl)[0]  # probs\n\n\n\n\n\n\n\n\n\npreds = preds_probs.max(axis=1).indices\nids = np.arange(1, preds.shape[0]+1)\nsubmit_df = pd.DataFrame(np.vstack((ids, preds)).T, columns=[\"ImageId\", \"Label\"])\n\n\nsubmit_df.to_csv(\"submission.csv\", index=False)\n\nWe only used 5% of the data for training and validation, to make it faster, so, after iterating this process enough to be confident of our data processing and validation methods, and our model’s architecture and hypeparameters, we would retrain and validate it on the whole dataset."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Pandas Tests\n\n\nBackup of tests with the Pandas data manipulation tool.\n\n\n\nTests\n\n\n\n\n\n\nAfonso Matoso Magalhães\n\n\n\n\n\n\n\n\n\n\n\n\nfastai: Image Classifier\n\n\nApplication of the fastai library to the MNIST dataset.\n\n\n\ndeep learning\n\n\nimage classification\n\n\n\n\n\n\nAfonso Matoso Magalhães\n\n\n\n\n\n\n\n\n\n\n\n\nSpark\n\n\nSimple and practical explanation of Spark.\n\n\n\nExplanation\n\n\n\n\n\n\nAfonso Matoso Magalhães\n\n\n\n\n\n\nNo matching items"
  }
]