[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/fastai-img-classif/fastai-img-classif.html",
    "href": "posts/fastai-img-classif/fastai-img-classif.html",
    "title": "fastai: Image Classifier",
    "section": "",
    "text": "This post uses the fastai library to classify images of the MNIST dataset (https://www.kaggle.com/competitions/digit-recognizer/data)."
  },
  {
    "objectID": "posts/fastai-img-classif/fastai-img-classif.html#load-data",
    "href": "posts/fastai-img-classif/fastai-img-classif.html#load-data",
    "title": "fastai: Image Classifier",
    "section": "Load Data",
    "text": "Load Data\n\nimport numpy as np\nimport pandas as pd\n\n\ntrain_df = pd.read_csv(\"./data/mnist/train.csv\")\ntest_df = pd.read_csv(\"./data/mnist/test.csv\")\n\n\ndisplay(train_df.head(2))\ndisplay(test_df.head(2))\n\n\n\n\n\n\n\n\n\nlabel\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 785 columns\n\n\n\n\n\n\n\n\n\n\n\n\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\npixel9\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 784 columns\n\n\n\n\n\ndef get_X_y(df: pd.DataFrame, \n            train: bool,\n            frac: float = None,\n            random_state: int = None):\n    \n    if train:\n        sample_df = df.groupby(\"label\").sample(frac=frac, random_state=random_state)\n        X, y = sample_df.iloc[:, 1:].values, sample_df.iloc[:, 0].values\n    else:\n        X = df.values\n        y = None\n    \n    X = X.reshape(-1, 28, 28).astype(np.uint8)\n    return np.moveaxis(np.stack((X,) * 3, axis=1), source=1, destination=-1), y\n\n\ntrain_X, train_y = get_X_y(train_df, train=True, frac=0.05, random_state=0)\ntest_X, _ = get_X_y(test_df, train=False)\n\n\nimport os\n\ntrain_dir = \"./data/mnist/train\"\ntest_dir = \"./data/mnist/test\"\n\nif not os.path.exists(train_dir):\n    os.mkdir(train_dir)\nif not os.path.exists(test_dir):\n    os.mkdir(test_dir)\n    \nfor label in np.unique(train_y):\n    if not os.path.exists(f\"{train_dir}/{label}\"):\n        os.mkdir(f\"{train_dir}/{label}\")\n\n\nfrom PIL import Image\n\nfor i in range(train_X.shape[0]): \n    Image.fromarray(train_X[i]).save(f\"{train_dir}/{train_y[i]}/{i}.jpg\")\n    \nfor i in range(test_X.shape[0]):\n    Image.fromarray(test_X[i]).save(f\"{test_dir}/{i}.jpg\")"
  },
  {
    "objectID": "posts/fastai-img-classif/fastai-img-classif.html#train-model",
    "href": "posts/fastai-img-classif/fastai-img-classif.html#train-model",
    "title": "fastai: Image Classifier",
    "section": "Train Model",
    "text": "Train Model\n\nfrom fastai.vision.all import *\n\n\ndls = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                get_items = get_image_files,\n                get_y = parent_label,\n                splitter = RandomSplitter()\n               ).dataloaders(train_dir)\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\ndls.n\n\n1681\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(10)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.928154\n1.347422\n0.430952\n00:07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.481526\n0.993762\n0.323810\n00:08\n\n\n1\n1.170102\n0.677061\n0.190476\n00:08\n\n\n2\n0.850052\n0.567972\n0.152381\n00:07\n\n\n3\n0.593134\n0.495614\n0.114286\n00:08\n\n\n4\n0.411312\n0.470167\n0.102381\n00:08\n\n\n5\n0.285538\n0.404801\n0.100000\n00:08\n\n\n6\n0.206531\n0.390356\n0.102381\n00:08\n\n\n7\n0.147591\n0.366526\n0.090476\n00:08\n\n\n8\n0.108263\n0.369601\n0.092857\n00:07\n\n\n9\n0.083111\n0.380087\n0.107143\n00:08\n\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5, nrows=1, figsize=(17,4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can correct images annotations or remove them, using the ImageClassifierCleaner. That will show the images for each class (in the train or validation set) where the trained model had the highest classification error. Let’s import the required modules:\n\nfrom fastai.vision.widgets import *\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEach time we make a change in one class’s and data set’s (train/validation) samples, by reannotating/removing it, we need to run the following lines before changing to another class or data set, so the changes are applied to the actual data:\n\nfor ind in cleaner.delete(): cleaner.fns[ind].unlink()\nfor ind, cat in cleaner.change(): shutil.move(str(cleaner.fns[ind]), f\"{train_dir}/{cat}\")\n\nIn the end of validating the data, we need to create a new DataLoaders object, to reflect the changes made to the dataset, and retrain the model.\n\ndls = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                get_items = get_image_files,\n                get_y = parent_label,\n                splitter = RandomSplitter()\n               ).dataloaders(train_dir)\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(10)\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.795555\n1.342019\n0.443914\n00:06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.438523\n0.962120\n0.300716\n00:08\n\n\n1\n1.135789\n0.664537\n0.210024\n00:08\n\n\n2\n0.830221\n0.450894\n0.133652\n00:08\n\n\n3\n0.584083\n0.362221\n0.124105\n00:08\n\n\n4\n0.418472\n0.378130\n0.121718\n00:08\n\n\n5\n0.285779\n0.320634\n0.095465\n00:08\n\n\n6\n0.194847\n0.321378\n0.093079\n00:08\n\n\n7\n0.141932\n0.309384\n0.090692\n00:08\n\n\n8\n0.099131\n0.295548\n0.088305\n00:07\n\n\n9\n0.078589\n0.292757\n0.090692\n00:08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndls.n\n\n1676"
  },
  {
    "objectID": "posts/fastai-img-classif/fastai-img-classif.html#test-model",
    "href": "posts/fastai-img-classif/fastai-img-classif.html#test-model",
    "title": "fastai: Image Classifier",
    "section": "Test Model",
    "text": "Test Model\n\npd.read_csv(\"./data/mnist/sample_submission.csv\").head()\n\n\n\n\n\n\n\n\n\nImageId\nLabel\n\n\n\n\n0\n1\n0\n\n\n1\n2\n0\n\n\n2\n3\n0\n\n\n3\n4\n0\n\n\n4\n5\n0\n\n\n\n\n\n\n\n\n\ntest_dl = dls.test_dl(get_image_files(test_dir))\npreds_probs = learn.get_preds(dl=test_dl)[0]  # probs\n\n\n\n\n\n\n\n\n\npreds = preds_probs.max(axis=1).indices\nids = np.arange(1, preds.shape[0]+1)\nsubmit_df = pd.DataFrame(np.vstack((ids, preds)).T, columns=[\"ImageId\", \"Label\"])\n\n\nsubmit_df.to_csv(\"./data/mnist/submission.csv\", index=False)\n\nWe only used 5% of the data for training and validation, to make it faster, so, after iterating this process enough to be confident of our data processing and validation methods, and our model’s architecture and hypeparameters, we would retrain and validate it on the whole dataset.\n\nfrom shutil import rmtree\n\nrmtree(train_dir)\nrmtree(test_dir)"
  },
  {
    "objectID": "posts/fastai-img-classif/fastai-img-classif.html#load-data-1",
    "href": "posts/fastai-img-classif/fastai-img-classif.html#load-data-1",
    "title": "fastai: Image Classifier",
    "section": "Load Data",
    "text": "Load Data\n\nimport pandas as pd\n\n\nlabels = pd.read_csv(\"./data/animals/train-labels.csv\")\n\n\nlabels.head(2)\n\n\n\n\n\n\n\n\n\nid\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\n\n\n0\nZJ000000\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nZJ000001\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n\nlabels[\"label\"] = labels.iloc[:, 1:].idxmax(axis=1).values\nlabels = labels[[\"id\", \"label\"]].set_index(\"id\")\n\n\ndisplay(labels.head(2))\nprint(labels.shape)\n\n\n\n\n\n\n\n\n\nlabel\n\n\nid\n\n\n\n\n\nZJ000000\nbird\n\n\nZJ000001\nmonkey_prosimian\n\n\n\n\n\n\n\n\n(16488, 1)\n\n\n\nlabels_sample = labels.groupby(\"label\").sample(frac=0.05)[\"label\"]\n\n\nlabels_sample\n\nid\nZJ010349    antelope_duiker\nZJ016091    antelope_duiker\nZJ009916    antelope_duiker\nZJ009220    antelope_duiker\nZJ007576    antelope_duiker\n                 ...       \nZJ006439             rodent\nZJ003230             rodent\nZJ013217             rodent\nZJ002723             rodent\nZJ002394             rodent\nName: label, Length: 826, dtype: object"
  },
  {
    "objectID": "posts/fastai-img-classif/fastai-img-classif.html#train-model-1",
    "href": "posts/fastai-img-classif/fastai-img-classif.html#train-model-1",
    "title": "fastai: Image Classifier",
    "section": "Train Model",
    "text": "Train Model\n\nfrom fastai.vision.all import *\n\n\ntrain_path = \"./data/animals/train/\"\ntest_path = \"./data/animals/test/\"\n\n\ndef get_items(path):\n\n    path = Path(path)\n    res = []\n\n    for filename in os.listdir(path):\n        if filename[:-4] in labels_sample.index:\n            res.append(path/filename)\n\n    return L(res)\n\n\ndls = DataBlock(blocks=(ImageBlock, CategoryBlock), \n                get_items=get_items,\n                get_y=lambda x: labels_sample.loc[x.name[:-4]], \n                splitter=RandomSplitter(),\n                item_tfms=Resize(128, method=\"squish\"),\n                batch_tfms=[Normalize.from_stats(*imagenet_stats)]\n               ).dataloaders(train_path)\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\ndls.n\n\n661\n\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(10)\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.981672\n1.726659\n0.581818\n00:10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.026164\n1.483618\n0.539394\n00:11\n\n\n1\n1.731047\n1.412682\n0.496970\n00:11\n\n\n2\n1.418197\n1.315527\n0.472727\n00:11\n\n\n3\n1.145188\n1.348643\n0.448485\n00:11\n\n\n4\n0.908982\n1.325396\n0.436364\n00:11\n\n\n5\n0.720561\n1.400179\n0.430303\n00:11\n\n\n6\n0.579397\n1.426558\n0.424242\n00:11\n\n\n7\n0.474904\n1.426744\n0.393939\n00:11\n\n\n8\n0.390903\n1.437192\n0.418182\n00:11\n\n\n9\n0.330643\n1.440059\n0.406061\n00:11\n\n\n\n\n\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(k=5, nrows=1, figsize=(22,4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom fastai.vision.widgets import *\n\ncleaner = ImageClassifierCleaner(learn)\ncleaner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI can’t see enough to validate the images."
  },
  {
    "objectID": "posts/fastai-img-classif/fastai-img-classif.html#test-model-1",
    "href": "posts/fastai-img-classif/fastai-img-classif.html#test-model-1",
    "title": "fastai: Image Classifier",
    "section": "Test Model",
    "text": "Test Model\n\npd.read_csv(\"./data/animals/sample-submission.csv\").head()\n\n\n\n\n\n\n\n\n\nid\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\n\n\n0\nZJ016488\n0.048233\n0.189185\n0.044914\n0.199588\n0.106118\n0.132915\n0.166410\n0.112637\n\n\n1\nZJ016489\n0.097078\n0.061400\n0.026409\n0.241530\n0.144344\n0.051780\n0.287811\n0.089648\n\n\n2\nZJ016490\n0.124658\n0.089101\n0.189225\n0.174494\n0.180540\n0.079995\n0.085672\n0.076314\n\n\n3\nZJ016491\n0.109966\n0.048397\n0.055598\n0.323600\n0.322356\n0.063252\n0.008160\n0.068671\n\n\n4\nZJ016492\n0.165742\n0.184610\n0.005431\n0.136806\n0.000389\n0.122078\n0.151521\n0.233423\n\n\n\n\n\n\n\n\n\ntest_dl = dls.test_dl(get_image_files(test_path))\npreds_probs = learn.get_preds(dl=test_dl)[0]\n\n\n\n\n\n\n\n\n\nids = np.array([item.name[:-4] for item in test_dl.dataset.items])\n\n\nlearn.dls.vocab\n\n['antelope_duiker', 'bird', 'blank', 'civet_genet', 'hog', 'leopard', 'monkey_prosimian', 'rodent']\n\n\n\nsubmission_df = pd.DataFrame(data=preds_probs, columns= learn.dls.vocab)\nsubmission_df.insert(0, \"id\", ids)\n\n\nsubmission_df.head()\n\n\n\n\n\n\n\n\n\nid\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\n\n\n0\nZJ018402\n0.015556\n0.156195\n0.009487\n0.000612\n0.006590\n0.064117\n0.743825\n0.003619\n\n\n1\nZJ017731\n0.157978\n0.035174\n0.605841\n0.000882\n0.012846\n0.015059\n0.170520\n0.001701\n\n\n2\nZJ020823\n0.160550\n0.003754\n0.053754\n0.040143\n0.722096\n0.001078\n0.010336\n0.008287\n\n\n3\nZJ018364\n0.817872\n0.016857\n0.109952\n0.001333\n0.007137\n0.009395\n0.015491\n0.021963\n\n\n4\nZJ017057\n0.259660\n0.005285\n0.002374\n0.008931\n0.319650\n0.051824\n0.263565\n0.088711"
  },
  {
    "objectID": "posts/pandas-tests/code.html",
    "href": "posts/pandas-tests/code.html",
    "title": "Pandas Tests",
    "section": "",
    "text": "df = pd.DataFrame(range(1000000))\n\n%timeit for _, row in df.iterrows(): ...\n%timeit for row in df.itertuples(): ...\n%timeit df.apply(lambda x: None, axis=1)\n\n20.7 s ± 425 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n357 ms ± 21.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n1.87 s ± 31.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nitertuples() is the faster way to iterate over the rows of a DF. Besides, it keeps the data types for each column, while the other methods return a row as a pd.Series object, where the data is stored in a np.ndarray, so the data type for every element will be the same."
  },
  {
    "objectID": "posts/pandas-tests/code.html#iterating-over-rows",
    "href": "posts/pandas-tests/code.html#iterating-over-rows",
    "title": "Pandas Tests",
    "section": "",
    "text": "df = pd.DataFrame(range(1000000))\n\n%timeit for _, row in df.iterrows(): ...\n%timeit for row in df.itertuples(): ...\n%timeit df.apply(lambda x: None, axis=1)\n\n20.7 s ± 425 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n357 ms ± 21.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n1.87 s ± 31.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nitertuples() is the faster way to iterate over the rows of a DF. Besides, it keeps the data types for each column, while the other methods return a row as a pd.Series object, where the data is stored in a np.ndarray, so the data type for every element will be the same."
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "Camera traps are a tool used by conservationists to study and monitor a wide range of ecologies while limiting human interference. However, they also generate a vast amount of data that quickly exceeds the capacity of humans to sift through. That’s where machine learning can help! Advances in computer vision can help automate tasks like species detection and identification, so that humans can spend more time learning from and protecting these ecologies.\nThis post walks through an initial approach for the Conservision Practice Area challenge on DrivenData, a practice competition where you identify animal species in a real world dataset of wildlife images from Tai National Park in Côte d’Ivoire. This is a practice competition designed to be accessible to participants at all levels. That makes it a great place to dive into the world of data science competitions and computer vision.\n\n\n\ncamera trap images\n\n\nWe will go through the following steps in order to train a PyTorch model that can be used to identify the species of animal in a given image: 1. Set up your environment (feel free to skip) 2. Download the data 3. Explore the data 4. Split into train and evaluation sets 5. Build the Model 6. Training 7. Evaluation 8. Create submission\nThe only pre-requisite is a basic familiarity with Python and some of the basic concepts behind deep learning. We’ll guide you step-by-step through the rest.\nLet’s get started!\n\n\nFeel free to skip this step if you already have an environment set up.\nThe folks on our team typically use conda to manage environments. Once you have conda installed you can create a new “conserviz” environment (name it whatever you like) with:\nconda create -n conserviz python=3.8\nThen we activate the new environment and install the required libraries with pip. The pip command below includes all the libraries we’ll need for this notebook. Launch a jupyter notebook from this new environment.\nconda activate conserviz\npip install pandas matplotlib Pillow tqdm scikit-learn torch torchvision\n\n\n\nDownload the competition data from the Data Download page. You’ll need to first register for the competition by clicking on “Compete” and agreeing to the rules.\nThe competition.zip file contains everything you need to take part in this competition, including this notebook benchmark.ipynb. Unzip the archive into a location of your choice. The file structure should look like this:\n├── benchmark.ipynb\n├── submission_format.csv\n├── test_features\n│   ├── ZJ000000.jpg\n│   ├── ZJ000001.jpg\n│   └── ...\n├── test_features.csv\n├── train_features\n│   ├── ZJ016488.jpg\n│   ├── ZJ016489.jpg\n│   └── ...\n├── train_features.csv\n└── train_labels.csv\nNext, let’s import some of the usual suspects:\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nRead in the train and test CSVs first and see what they look like.\n\ntrain_features = pd.read_csv(\"train_features.csv\", index_col=\"id\")\ntest_features = pd.read_csv(\"test_features.csv\", index_col=\"id\")\ntrain_labels = pd.read_csv(\"train_labels.csv\", index_col=\"id\")\n\nThe features CSVs contain the image ID, filepath and site ID for each image.\n\ntrain_features.head()\n\n\n\n\n\n\n\n\n\nfilepath\nsite\n\n\nid\n\n\n\n\n\n\nZJ000000\ntrain_features/ZJ000000.jpg\nS0120\n\n\nZJ000001\ntrain_features/ZJ000001.jpg\nS0069\n\n\nZJ000002\ntrain_features/ZJ000002.jpg\nS0009\n\n\nZJ000003\ntrain_features/ZJ000003.jpg\nS0008\n\n\nZJ000004\ntrain_features/ZJ000004.jpg\nS0036\n\n\n\n\n\n\n\n\n\ntest_features.head()\n\n\n\n\n\n\n\n\n\nfilepath\nsite\n\n\nid\n\n\n\n\n\n\nZJ016488\ntest_features/ZJ016488.jpg\nS0082\n\n\nZJ016489\ntest_features/ZJ016489.jpg\nS0040\n\n\nZJ016490\ntest_features/ZJ016490.jpg\nS0040\n\n\nZJ016491\ntest_features/ZJ016491.jpg\nS0041\n\n\nZJ016492\ntest_features/ZJ016492.jpg\nS0040\n\n\n\n\n\n\n\n\nThe train_labels CSV is an indicator matrix of the species identified in each of the training images. Some images are labeled as “blank” if no animal was detected.\n\ntrain_labels.head()\n\n\n\n\n\n\n\n\n\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\nZJ000000\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZJ000001\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nZJ000002\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZJ000003\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nZJ000004\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nLet’s store a sorted list of the labels, so that we can sort the inputs and outputs to our model in a consistent way.\n\nspecies_labels = sorted(train_labels.columns.unique())\nspecies_labels\n\n['antelope_duiker',\n 'bird',\n 'blank',\n 'civet_genet',\n 'hog',\n 'leopard',\n 'monkey_prosimian',\n 'rodent']\n\n\n\n\n\nNow let’s see what some of the actual images look like. The code below iterates through a list of species and selects a single random image from each species to display, along with its image ID and label. You can try changing the random_state variable to display a new set of images.\n\nimport matplotlib.image as mpimg\n\nrandom_state = 42\n\n# we'll create a grid with 8 positions, one for each label (7 species, plus blanks)\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(20, 20))\n\n# iterate through each species\nfor species, ax in zip(species_labels, axes.flat):\n    # get an image ID for this species\n    img_id = (\n        train_labels[train_labels.loc[:,species] == 1]\n        .sample(1, random_state=random_state)\n        .index[0]\n    )\n    # reads the filepath and returns a numpy array\n    img = mpimg.imread(train_features.loc[img_id].filepath)\n    # plot etc\n    ax.imshow(img)\n    ax.set_title(f\"{img_id} | {species}\")\n\n\n\n\n\n\n\n\nCan you spot the animals? I’m still not sure where the rodent is. Birds can be tough to spot too.\nLet’s look at the distribution of species across the training set, first in terms of overall counts and then in percentage terms.\n\ntrain_labels.sum().sort_values(ascending=False)\n\nmonkey_prosimian    2492.0\nantelope_duiker     2474.0\ncivet_genet         2423.0\nleopard             2254.0\nblank               2213.0\nrodent              2013.0\nbird                1641.0\nhog                  978.0\ndtype: float64\n\n\n\ntrain_labels.sum().divide(train_labels.shape[0]).sort_values(ascending=False)\n\nmonkey_prosimian    0.151140\nantelope_duiker     0.150049\ncivet_genet         0.146955\nleopard             0.136705\nblank               0.134219\nrodent              0.122089\nbird                0.099527\nhog                 0.059316\ndtype: float64\n\n\nIn case you’re curious, this distribution is not exactly what we find in the wild. The competition dataset has been curated a little bit to produce a more uniform distribution than we would see in the actual data.\nThere’s a lot more data exploration to do. For example, you might also want to look at the distribution of image dimensions or camera trap sites. But since our primary goal here is to develop a benchmark, let’s move on to the modeling!\n\n\n\nFirst, we’ll need to split the images into train and eval sets. We’ll put aside 25% of the data for evaluation and stratify by the target labels to ensure we have similar relative frequencies of each class in the train and eval sets.\nFor the purposes of this benchmark, we’re also going to limit ourselves to a 50% subset of the training data, just so that things run faster. But feel free to adjust frac or remove it entirely if you want to run the training on the full set.\n\nfrom sklearn.model_selection import train_test_split\n\nfrac = 0.5\n\ny = train_labels.sample(frac=frac, random_state=1)\nx = train_features.loc[y.index].filepath.to_frame()\n\n# note that we are casting the species labels to an indicator/dummy matrix\nx_train, x_eval, y_train, y_eval = train_test_split(\n    x, y, stratify=y, test_size=0.25\n)\n\nHere’s what x_train and y_train look like now:\n\nx_train.head()\n\n\n\n\n\n\n\n\n\nfilepath\n\n\nid\n\n\n\n\n\nZJ002477\ntrain_features/ZJ002477.jpg\n\n\nZJ012222\ntrain_features/ZJ012222.jpg\n\n\nZJ013173\ntrain_features/ZJ013173.jpg\n\n\nZJ000959\ntrain_features/ZJ000959.jpg\n\n\nZJ008167\ntrain_features/ZJ008167.jpg\n\n\n\n\n\n\n\n\n\ny_train.head()\n\n\n\n\n\n\n\n\n\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\nZJ002477\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZJ012222\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nZJ013173\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZJ000959\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nZJ008167\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nx_train.shape, y_train.shape, x_eval.shape, y_eval.shape\n\n((6183, 1), (6183, 8), (2061, 1), (2061, 8))\n\n\nNext, let’s validate that our split has resulted in roughly similar relative distributions of species across the train and eval sets (because of how we passed stratify=y above).\n\nsplit_pcts = pd.DataFrame(\n    {\n        \"train\": y_train.idxmax(axis=1).value_counts(normalize=True),\n        \"eval\": y_eval.idxmax(axis=1).value_counts(normalize=True),\n    }\n)\nprint(\"Species percentages by split\")\n(split_pcts.fillna(0) * 100).astype(int)\n\nSpecies percentages by split\n\n\n\n\n\n\n\n\n\n\ntrain\neval\n\n\n\n\nmonkey_prosimian\n15\n15\n\n\nantelope_duiker\n14\n14\n\n\ncivet_genet\n14\n14\n\n\nblank\n13\n13\n\n\nleopard\n13\n13\n\n\nrodent\n11\n11\n\n\nbird\n9\n9\n\n\nhog\n5\n5\n\n\n\n\n\n\n\n\nGood, this looks as expected.\n\n\n\nNow we can start building our model.\n\n\nFirst, we’ll create an ImagesDataset class that will define how we access our data and any transformations we might want to apply.\nThis new class will inherit from the PyTorch Dataset class, but we’ll also need to define our own __init__, __len__ and __getitem__ special methods: * __init__ will instantiate the dataset object with two dataframes: an x_train df containing image IDs and image file paths, and a y_train df containing image IDs and labels. This will run once when we first create the dataset object, e.g. with dataset = ImagesDataset(x_train, y_train). * __getitem__ will define how we access a sample from the data. This method gets called whenever we use an indexing operation like dataset[index]. In this case, whenever accessing a particular image sample (for example, to get the first image we’d do dataset[0]) the following will happen: * look up the image filepath using the index * load the image with PIL.Image * apply some transformations (more on this below) * return a dictionary containing the image ID, the image itself as a Tensor, and a label (if it exists) * __len__ simply returns the size of the dataset, which we do by calling len on the input dataframe.\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\n\nclass ImagesDataset(Dataset):\n    \"\"\"Reads in an image, transforms pixel values, and serves\n    a dictionary containing the image id, image tensors, and label.\n    \"\"\"\n\n    def __init__(self, x_df, y_df=None):\n        self.data = x_df\n        self.label = y_df\n        self.transform = transforms.Compose(\n            [\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n                ),\n            ]\n        )\n\n    def __getitem__(self, index):\n        image = Image.open(self.data.iloc[index][\"filepath\"]).convert(\"RGB\")\n        image = self.transform(image)\n        image_id = self.data.index[index]\n        # if we don't have labels (e.g. for test set) just return the image and image id\n        if self.label is None:\n            sample = {\"image_id\": image_id, \"image\": image}\n        else:\n            label = torch.tensor(self.label.iloc[index].values, \n                                 dtype=torch.float)\n            sample = {\"image_id\": image_id, \"image\": image, \"label\": label}\n        return sample\n\n    def __len__(self):\n        return len(self.data)\n\nNotice that we are also defining a set of transformations, which are defined in the __init__ and called in the __getitem__ special methods. These are applied to each image before returning it. Here’s what each of those transformations do and why:\n\ntransforms.Resize((224, 224)) ResNet50 was trained on images of size 224x224 so we resize to the same dimensions here. See pytorch docs and the ResNet paper.\ntransforms.ToTensor() converts the image to a tensor. Since we are passing in a PIL Image at this point, PyTorch can recognize it as an RGB image and will automatically convert the input values which are in the range [0, 255] to a range of [0, 1]. See more from the PyTorch docs.\ntransforms.Normalize(...) normalizes the image tensors using the mean and standard deviation of ImageNet images. Because this transformation was applied to images when training the ResNet model, we want to do the same here with our images. See more from the PyTorch docs on pretrained models.\n\n\n\n\nNext, we need to load the dataset into a dataloader. The DataLoader class lets us iterate through our dataset in batches.\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataset = ImagesDataset(x_train, y_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32)\n\nThe data pieces are now largely in place!\n\n\n\n\nNow it’s time to start building our model and then training it.\nWe’ll use a pretrained ResNet50 model as our backbone. ResNets are one of the more popular networks for image classification tasks. The pretrained model outputs a 2048-dimension embedding, which we will then connect to two more dense layers, with a ReLU and Dropout step in between.\nThese final layers, defined in model.fc, are the new “head” of our model, and allow us to transform the image embeddings produced by the pretrained “backbone” into the 8-dimensional output required to learn the species classification task we’re tackling here. Prior to redefining it below, model.fc would be the final, dense layer connecting the 2048-dimension embedding to a 1000-dimension output (corresponding to the 1000 ImageNet classes that the pretrained model was trained on). We will instead prepare the model for the current task by redefining model.fc to produce an 8-dimensional output corresponding to our 8 species classes (including blanks).\nWe’ll also add a couple more layers in between. The ReLU layer introduces non-linearity into the model head, in effect activating important features and suppressing noise. And the Dropout layer is a commonly used regularization component that randomly drops some nodes from the previous layer’s outputs (10% of nodes in this case) during each training step, mitigating our risk of overfitting.\n\nfrom torch import nn\nimport torchvision.models as models\n\n\nmodel = models.resnet50(pretrained=True)\nmodel.fc = nn.Sequential(\n    nn.Linear(2048, 100),  # dense layer takes a 2048-dim input and outputs 100-dim\n    nn.ReLU(inplace=True),  # ReLU activation introduces non-linearity\n    nn.Dropout(0.1),  # common technique to mitigate overfitting\n    nn.Linear(\n        100, 8\n    ),  # final dense layer outputs 8-dim corresponding to our target classes\n)\n\n\n\nCross entropy loss (or log loss) is a commonly used loss function for multi-class (not multi-label) image classification. We’ll use this to compute loss for each training batch and then update our parameters accordingly.\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n\n\n\nWe’re now ready to train our model!\nWe’ll start simple and just run it for one epoch, but feel free to run it for more num_epochs if you’ve got the time. We hope to see a decreasing loss as training progresses, which will provide some evidence that the model is learning. Note that we haven’t frozen any weights in the pretrained model, a choice which you may want to revisit and we discuss in a little more detail below.\nFor each epoch we’ll iterate through the batches, and for each batch we’ll do the following: 1. Zero out the gradients. PyTorch will sum the gradients from past batches when doing its backward pass, so in order to make sure we are only using the gradients computed for the current batch, we zero out the gradients at the beginning of each batch. 2. Run the forward pass. 3. Compute the loss and track it. 4. Compute our gradients and update our weight parameters.\n\nnum_epochs = 1\n\ntracking_loss = {}\n\nfor epoch in range(1, num_epochs + 1):\n    print(f\"Starting epoch {epoch}\")\n\n    # iterate through the dataloader batches. tqdm keeps track of progress.\n    for batch_n, batch in tqdm(\n        enumerate(train_dataloader), total=len(train_dataloader)\n    ):\n\n        # 1) zero out the parameter gradients so that gradients from previous batches are not used in this step\n        optimizer.zero_grad()\n\n        # 2) run the foward step on this batch of images\n        outputs = model(batch[\"image\"])\n\n        # 3) compute the loss\n        loss = criterion(outputs, batch[\"label\"])\n        # let's keep track of the loss by epoch and batch\n        tracking_loss[(epoch, batch_n)] = float(loss)\n\n        # 4) compute our gradients\n        loss.backward()\n        # update our weights\n        optimizer.step()\n\nStarting epoch 1\n\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 194/194 [35:46&lt;00:00, 11.06s/it]\n\n\nNow let’s plot the loss by epoch and batch. The x-axis here is a tuple of (epoch, batch).\n\ntracking_loss = pd.Series(tracking_loss)\n\nplt.figure(figsize=(10, 5))\ntracking_loss.plot(alpha=0.2, label=\"loss\")\ntracking_loss.rolling(center=True, min_periods=1, window=10).mean().plot(\n    label=\"loss (moving avg)\"\n)\nplt.xlabel(\"(Epoch, Batch)\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=0)\n\n\n\n\n\n\n\n\nGood news, the loss is going down! This is an encouraging start, especially since we haven’t done anything fancy yet.\n\n\n\nWe have the model loaded in memory already, so we don’t really need to save the model, but it’s often useful to do this so we can use it again later.\nHere’s how:\n\ntorch.save(model, \"model.pth\")\n\n\n\n\n\nSo far, not so bad. We’ve shown an improvement in the loss on the training set, but that tells us little about how our model will do on new data. Let’s reload our saved model and try generating some predictions on the evaluation split we created earlier.\n\nloaded_model = torch.load(\"model.pth\")\n\nWe create the eval dataset and dataloader just like we did earlier with the training dataset and dataloader:\n\neval_dataset = ImagesDataset(x_eval, y_eval)\neval_dataloader = DataLoader(eval_dataset, batch_size=32)\n\n\n\nWe’ll iterate through the eval dataloader in batches, just like we did for training, but this time we aren’t going to need to compute gradients or update weights. For each batch, we’ll do the following: 1. Run the forward pass to get the model output or logits 2. Apply a softmax function to convert the logits into probability space with range[0,1]. During training, the softmax operation was handled internally by nn.CrossEntropyLoss. We aren’t computing the loss now because we are just doing evaluation, but we still want the predictions to be in the range[0,1]. 3. Store the results in a dataframe for further analysis\n\npreds_collector = []\n\n# put the model in eval mode so we don't update any parameters\nmodel.eval()\n\n# we aren't updating our weights so no need to calculate gradients\nwith torch.no_grad():\n    for batch in tqdm(eval_dataloader, total=len(eval_dataloader)):\n        # 1) run the forward step\n        logits = model.forward(batch[\"image\"])\n        # 2) apply softmax so that model outputs are in range [0,1]\n        preds = nn.functional.softmax(logits, dim=1)\n        # 3) store this batch's predictions in df\n        # note that PyTorch Tensors need to first be detached from their computational graph before converting to numpy arrays\n        preds_df = pd.DataFrame(\n            preds.detach().numpy(),\n            index=batch[\"image_id\"],\n            columns=species_labels,\n        )\n        preds_collector.append(preds_df)\n\neval_preds_df = pd.concat(preds_collector)\neval_preds_df\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [03:59&lt;00:00,  3.68s/it]\n\n\n\n\n\n\n\n\n\n\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\n\n\nZJ005376\n0.171700\n0.192484\n0.176443\n0.009301\n0.030189\n0.065699\n0.318819\n0.035365\n\n\nZJ011044\n0.001978\n0.002307\n0.001948\n0.000101\n0.001681\n0.990360\n0.000906\n0.000719\n\n\nZJ005242\n0.210068\n0.189871\n0.121386\n0.012617\n0.021859\n0.031385\n0.358011\n0.054803\n\n\nZJ004518\n0.238102\n0.253908\n0.109069\n0.008890\n0.023841\n0.037896\n0.283491\n0.044803\n\n\nZJ000101\n0.283641\n0.121174\n0.159294\n0.024840\n0.041801\n0.041539\n0.260422\n0.067289\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nZJ011868\n0.130162\n0.207689\n0.163008\n0.021061\n0.053528\n0.120070\n0.230920\n0.073561\n\n\nZJ002183\n0.247485\n0.180399\n0.125990\n0.009920\n0.018781\n0.022285\n0.352043\n0.043098\n\n\nZJ014186\n0.069278\n0.339674\n0.087901\n0.004488\n0.021389\n0.341253\n0.110601\n0.025416\n\n\nZJ011633\n0.223148\n0.143238\n0.156081\n0.032510\n0.046612\n0.049409\n0.277509\n0.071493\n\n\nZJ001374\n0.006084\n0.005903\n0.006707\n0.000377\n0.004165\n0.968445\n0.006105\n0.002214\n\n\n\n\n2061 rows × 8 columns\n\n\n\n\n\n\n\nFirst let’s review the species distribution we saw in the training set.\n\nprint(\"True labels (training):\")\ny_train.idxmax(axis=1).value_counts()\n\nTrue labels (training):\n\n\nmonkey_prosimian    973\nantelope_duiker     925\ncivet_genet         896\nblank               860\nleopard             841\nrodent              732\nbird                608\nhog                 348\ndtype: int64\n\n\nHere’s the distribution of our predictions on the eval set.\n\nprint(\"Predicted labels (eval):\")\neval_preds_df.idxmax(axis=1).value_counts()\n\nPredicted labels (eval):\n\n\nmonkey_prosimian    919\ncivet_genet         403\nleopard             329\nrodent              132\nblank               122\nantelope_duiker     106\nbird                 48\nhog                   2\ndtype: int64\n\n\nThe actual evaluation set is more evenly distributed than our predictions, so we already know there is some room for improvement here.\n\nprint(\"True labels (eval):\")\ny_eval.idxmax(axis=1).value_counts()\n\nTrue labels (eval):\n\n\nmonkey_prosimian    325\nantelope_duiker     308\ncivet_genet         298\nblank               287\nleopard             280\nrodent              244\nbird                203\nhog                 116\ndtype: int64\n\n\n\n\n\nNow let’s compute how accurate our model is and compare that against some trivial baseline models. First let’s get the labels with the highest score for each image.\n\neval_predictions = eval_preds_df.idxmax(axis=1)\neval_predictions.head()\n\nZJ005376    monkey_prosimian\nZJ011044             leopard\nZJ005242    monkey_prosimian\nZJ004518    monkey_prosimian\nZJ000101     antelope_duiker\ndtype: object\n\n\nRandom guessing across 8 classes would yield an accuracy of 12.5% (1/8). But we could construct a slightly better trivial model by always guessing the most common class (“monkey_prosimian” images in this case).\nIf we were to always guess that an image is monkey_prosimian, we could achieve accuracy of 15.8%.\n\neval_true = y_eval.idxmax(axis=1)\n\n(eval_true == \"monkey_prosimian\").sum() / len(eval_predictions)\n\n0.1576904415332363\n\n\nLet’s see how our model compares. We take the species with the highest score for each image (eval_predictions) and compare that to the true labels.\n\ncorrect = (eval_predictions == eval_true).sum()\naccuracy = correct / len(eval_predictions)\naccuracy\n\n0.49199417758369723\n\n\nOur accuracy on the evaluation set is about 50%, which is not a bad start for a very simple first pass and one epoch of training.\nLet’s look at the predictions from another angle.\nWe can see from the confusion matrix below that our model does reasonably well on some species, but we have plenty of room for improvement on antelopes, birds, hogs and blanks.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfig, ax = plt.subplots(figsize=(10, 10))\ncm = ConfusionMatrixDisplay.from_predictions(\n    y_eval.idxmax(axis=1),\n    eval_preds_df.idxmax(axis=1),\n    ax=ax,\n    xticks_rotation=90,\n    colorbar=True,\n)\n\n\n\n\n\n\n\n\nThat’s where you come in! What can you do to improve on this benchmark?\nHere are some ideas you might want to try: * Train on the full training dataset. We’ve only used 50% of the training data so far. * Train for more epochs. We’ve only done 1 so far. * Try another pretrained model. For example, you may have more success with EfficientNet, or another ResNet model with more layers like ResNet152. See what’s available from pytorch here. You may also want to review which models are or have been state of the art for image classification tasks, for example on paperswithcode.com. Keep in mind that different models will require different input and output dimensions, so you’ll need to update how you construct model above. * Experiment with different loss functions. * Experiment with different learning rates or learning rate schedulers. * Add more layers to the model head (model.fc). * You also may want to consider freezing the weights in the backbone model and only training the head (model.fc). If this results in higher accuracy, that suggests the current approach may be overwriting the backbone weights in a problematic way. One approach here would be to train just the model head, and then unfreeze the backbone but train at a lower learning rate. * Training will be much faster using GPUs, but you will need to make some small tweaks to the code. * As you become more comfortable iterating through different versions of the model, you may want to try out PyTorch Lightning or Lightning Flash, which build upon PyTorch and eliminate a lot of boilerplate code, in addition to providing a more complete research framework for deep learning problems.\n\n\n\n\nLast but not least, we’ll want to participate in the competition and see where we stand on the leaderboard.\nTo do this we need to create predictions for the competition test set (not the eval set we used above). You don’t have labels for these.\nWe’ll create predictions in the same way we did for the eval set, but this time using the test_features we downloaded from the competition website.\n\ntest_dataset = ImagesDataset(test_features.filepath.to_frame())\ntest_dataloader = DataLoader(test_dataset, batch_size=32)\n\n\npreds_collector = []\n\n# put the model in eval mode so we don't update any parameters\nmodel.eval()\n\n# we aren't updating our weights so no need to calculate gradients\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n        # run the forward step\n        logits = model.forward(batch[\"image\"])\n        # apply softmax so that model outputs are in range [0,1]\n        preds = nn.functional.softmax(logits, dim=1)\n        # store this batch's predictions in df\n        # note that PyTorch Tensors need to first be detached from their computational graph before converting to numpy arrays\n        preds_df = pd.DataFrame(\n            preds.detach().numpy(),\n            index=batch[\"image_id\"],\n            columns=species_labels,\n        )\n        preds_collector.append(preds_df)\n\nsubmission_df = pd.concat(preds_collector)\nsubmission_df\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140/140 [09:19&lt;00:00,  4.00s/it]\n\n\n\n\n\n\n\n\n\n\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\n\n\nZJ016488\n0.040287\n0.016456\n0.069033\n0.639093\n0.018142\n0.018959\n0.015982\n0.182047\n\n\nZJ016489\n0.257348\n0.117252\n0.138006\n0.024684\n0.049720\n0.057412\n0.294213\n0.061365\n\n\nZJ016490\n0.242068\n0.075779\n0.177219\n0.098522\n0.061314\n0.060584\n0.160722\n0.123793\n\n\nZJ016491\n0.009359\n0.010228\n0.009832\n0.000861\n0.008720\n0.952021\n0.005001\n0.003978\n\n\nZJ016492\n0.242449\n0.117502\n0.119074\n0.007304\n0.017310\n0.022663\n0.443947\n0.029752\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nZJ020947\n0.176772\n0.180421\n0.153451\n0.008706\n0.040353\n0.151517\n0.258835\n0.029944\n\n\nZJ020948\n0.270863\n0.149436\n0.135711\n0.009466\n0.028289\n0.054232\n0.311244\n0.040758\n\n\nZJ020949\n0.072901\n0.022102\n0.097448\n0.489528\n0.043097\n0.028081\n0.024680\n0.222163\n\n\nZJ020950\n0.167972\n0.235275\n0.123283\n0.008515\n0.028146\n0.047482\n0.340371\n0.048957\n\n\nZJ020951\n0.002522\n0.006116\n0.004133\n0.000110\n0.002457\n0.980648\n0.003158\n0.000856\n\n\n\n\n4464 rows × 8 columns\n\n\n\n\nLet’s check a couple things on submission_df before submitting to the platform. We’ll want to make sure our submission’s index and column labels match the submission format. (The DrivenData platform will do these data integrity checks as well, but it will be quicker to detect problems this way.)\n\nsubmission_format = pd.read_csv(\"submission_format.csv\", index_col=\"id\")\n\nassert all(submission_df.index == submission_format.index)\nassert all(submission_df.columns == submission_format.columns)\n\nLooks like we’re ready to submit! Save the dataframe out to a CSV file and then upload it via the Submissions page on the competition website.\n\nsubmission_df.to_csv(\"submission_df.csv\")\n\nHow did we do? We should get a score of ~1.8, though your results may differ slightly due to non-determinism in model training. (For reference, a randomly generated submission yields a score of something like ~2.4.)\n\n\n\nbenchmark submission\n\n\nNow it is up to you to improve on this benchmark!\nHead over to the competition for data and more background info, or the competition forum if you have any questions. Good luck!"
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html#set-up-your-environment",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html#set-up-your-environment",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "Feel free to skip this step if you already have an environment set up.\nThe folks on our team typically use conda to manage environments. Once you have conda installed you can create a new “conserviz” environment (name it whatever you like) with:\nconda create -n conserviz python=3.8\nThen we activate the new environment and install the required libraries with pip. The pip command below includes all the libraries we’ll need for this notebook. Launch a jupyter notebook from this new environment.\nconda activate conserviz\npip install pandas matplotlib Pillow tqdm scikit-learn torch torchvision"
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html#download-the-data",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html#download-the-data",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "Download the competition data from the Data Download page. You’ll need to first register for the competition by clicking on “Compete” and agreeing to the rules.\nThe competition.zip file contains everything you need to take part in this competition, including this notebook benchmark.ipynb. Unzip the archive into a location of your choice. The file structure should look like this:\n├── benchmark.ipynb\n├── submission_format.csv\n├── test_features\n│   ├── ZJ000000.jpg\n│   ├── ZJ000001.jpg\n│   └── ...\n├── test_features.csv\n├── train_features\n│   ├── ZJ016488.jpg\n│   ├── ZJ016489.jpg\n│   └── ...\n├── train_features.csv\n└── train_labels.csv\nNext, let’s import some of the usual suspects:\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nRead in the train and test CSVs first and see what they look like.\n\ntrain_features = pd.read_csv(\"train_features.csv\", index_col=\"id\")\ntest_features = pd.read_csv(\"test_features.csv\", index_col=\"id\")\ntrain_labels = pd.read_csv(\"train_labels.csv\", index_col=\"id\")\n\nThe features CSVs contain the image ID, filepath and site ID for each image.\n\ntrain_features.head()\n\n\n\n\n\n\n\n\n\nfilepath\nsite\n\n\nid\n\n\n\n\n\n\nZJ000000\ntrain_features/ZJ000000.jpg\nS0120\n\n\nZJ000001\ntrain_features/ZJ000001.jpg\nS0069\n\n\nZJ000002\ntrain_features/ZJ000002.jpg\nS0009\n\n\nZJ000003\ntrain_features/ZJ000003.jpg\nS0008\n\n\nZJ000004\ntrain_features/ZJ000004.jpg\nS0036\n\n\n\n\n\n\n\n\n\ntest_features.head()\n\n\n\n\n\n\n\n\n\nfilepath\nsite\n\n\nid\n\n\n\n\n\n\nZJ016488\ntest_features/ZJ016488.jpg\nS0082\n\n\nZJ016489\ntest_features/ZJ016489.jpg\nS0040\n\n\nZJ016490\ntest_features/ZJ016490.jpg\nS0040\n\n\nZJ016491\ntest_features/ZJ016491.jpg\nS0041\n\n\nZJ016492\ntest_features/ZJ016492.jpg\nS0040\n\n\n\n\n\n\n\n\nThe train_labels CSV is an indicator matrix of the species identified in each of the training images. Some images are labeled as “blank” if no animal was detected.\n\ntrain_labels.head()\n\n\n\n\n\n\n\n\n\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\nZJ000000\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZJ000001\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nZJ000002\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZJ000003\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nZJ000004\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\nLet’s store a sorted list of the labels, so that we can sort the inputs and outputs to our model in a consistent way.\n\nspecies_labels = sorted(train_labels.columns.unique())\nspecies_labels\n\n['antelope_duiker',\n 'bird',\n 'blank',\n 'civet_genet',\n 'hog',\n 'leopard',\n 'monkey_prosimian',\n 'rodent']"
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html#explore-the-data",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html#explore-the-data",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "Now let’s see what some of the actual images look like. The code below iterates through a list of species and selects a single random image from each species to display, along with its image ID and label. You can try changing the random_state variable to display a new set of images.\n\nimport matplotlib.image as mpimg\n\nrandom_state = 42\n\n# we'll create a grid with 8 positions, one for each label (7 species, plus blanks)\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(20, 20))\n\n# iterate through each species\nfor species, ax in zip(species_labels, axes.flat):\n    # get an image ID for this species\n    img_id = (\n        train_labels[train_labels.loc[:,species] == 1]\n        .sample(1, random_state=random_state)\n        .index[0]\n    )\n    # reads the filepath and returns a numpy array\n    img = mpimg.imread(train_features.loc[img_id].filepath)\n    # plot etc\n    ax.imshow(img)\n    ax.set_title(f\"{img_id} | {species}\")\n\n\n\n\n\n\n\n\nCan you spot the animals? I’m still not sure where the rodent is. Birds can be tough to spot too.\nLet’s look at the distribution of species across the training set, first in terms of overall counts and then in percentage terms.\n\ntrain_labels.sum().sort_values(ascending=False)\n\nmonkey_prosimian    2492.0\nantelope_duiker     2474.0\ncivet_genet         2423.0\nleopard             2254.0\nblank               2213.0\nrodent              2013.0\nbird                1641.0\nhog                  978.0\ndtype: float64\n\n\n\ntrain_labels.sum().divide(train_labels.shape[0]).sort_values(ascending=False)\n\nmonkey_prosimian    0.151140\nantelope_duiker     0.150049\ncivet_genet         0.146955\nleopard             0.136705\nblank               0.134219\nrodent              0.122089\nbird                0.099527\nhog                 0.059316\ndtype: float64\n\n\nIn case you’re curious, this distribution is not exactly what we find in the wild. The competition dataset has been curated a little bit to produce a more uniform distribution than we would see in the actual data.\nThere’s a lot more data exploration to do. For example, you might also want to look at the distribution of image dimensions or camera trap sites. But since our primary goal here is to develop a benchmark, let’s move on to the modeling!"
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html#split-into-train-and-evaluation-sets",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html#split-into-train-and-evaluation-sets",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "First, we’ll need to split the images into train and eval sets. We’ll put aside 25% of the data for evaluation and stratify by the target labels to ensure we have similar relative frequencies of each class in the train and eval sets.\nFor the purposes of this benchmark, we’re also going to limit ourselves to a 50% subset of the training data, just so that things run faster. But feel free to adjust frac or remove it entirely if you want to run the training on the full set.\n\nfrom sklearn.model_selection import train_test_split\n\nfrac = 0.5\n\ny = train_labels.sample(frac=frac, random_state=1)\nx = train_features.loc[y.index].filepath.to_frame()\n\n# note that we are casting the species labels to an indicator/dummy matrix\nx_train, x_eval, y_train, y_eval = train_test_split(\n    x, y, stratify=y, test_size=0.25\n)\n\nHere’s what x_train and y_train look like now:\n\nx_train.head()\n\n\n\n\n\n\n\n\n\nfilepath\n\n\nid\n\n\n\n\n\nZJ002477\ntrain_features/ZJ002477.jpg\n\n\nZJ012222\ntrain_features/ZJ012222.jpg\n\n\nZJ013173\ntrain_features/ZJ013173.jpg\n\n\nZJ000959\ntrain_features/ZJ000959.jpg\n\n\nZJ008167\ntrain_features/ZJ008167.jpg\n\n\n\n\n\n\n\n\n\ny_train.head()\n\n\n\n\n\n\n\n\n\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\nZJ002477\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZJ012222\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\nZJ013173\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\nZJ000959\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\nZJ008167\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nx_train.shape, y_train.shape, x_eval.shape, y_eval.shape\n\n((6183, 1), (6183, 8), (2061, 1), (2061, 8))\n\n\nNext, let’s validate that our split has resulted in roughly similar relative distributions of species across the train and eval sets (because of how we passed stratify=y above).\n\nsplit_pcts = pd.DataFrame(\n    {\n        \"train\": y_train.idxmax(axis=1).value_counts(normalize=True),\n        \"eval\": y_eval.idxmax(axis=1).value_counts(normalize=True),\n    }\n)\nprint(\"Species percentages by split\")\n(split_pcts.fillna(0) * 100).astype(int)\n\nSpecies percentages by split\n\n\n\n\n\n\n\n\n\n\ntrain\neval\n\n\n\n\nmonkey_prosimian\n15\n15\n\n\nantelope_duiker\n14\n14\n\n\ncivet_genet\n14\n14\n\n\nblank\n13\n13\n\n\nleopard\n13\n13\n\n\nrodent\n11\n11\n\n\nbird\n9\n9\n\n\nhog\n5\n5\n\n\n\n\n\n\n\n\nGood, this looks as expected."
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html#build-the-model",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html#build-the-model",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "Now we can start building our model.\n\n\nFirst, we’ll create an ImagesDataset class that will define how we access our data and any transformations we might want to apply.\nThis new class will inherit from the PyTorch Dataset class, but we’ll also need to define our own __init__, __len__ and __getitem__ special methods: * __init__ will instantiate the dataset object with two dataframes: an x_train df containing image IDs and image file paths, and a y_train df containing image IDs and labels. This will run once when we first create the dataset object, e.g. with dataset = ImagesDataset(x_train, y_train). * __getitem__ will define how we access a sample from the data. This method gets called whenever we use an indexing operation like dataset[index]. In this case, whenever accessing a particular image sample (for example, to get the first image we’d do dataset[0]) the following will happen: * look up the image filepath using the index * load the image with PIL.Image * apply some transformations (more on this below) * return a dictionary containing the image ID, the image itself as a Tensor, and a label (if it exists) * __len__ simply returns the size of the dataset, which we do by calling len on the input dataframe.\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\n\nclass ImagesDataset(Dataset):\n    \"\"\"Reads in an image, transforms pixel values, and serves\n    a dictionary containing the image id, image tensors, and label.\n    \"\"\"\n\n    def __init__(self, x_df, y_df=None):\n        self.data = x_df\n        self.label = y_df\n        self.transform = transforms.Compose(\n            [\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n                ),\n            ]\n        )\n\n    def __getitem__(self, index):\n        image = Image.open(self.data.iloc[index][\"filepath\"]).convert(\"RGB\")\n        image = self.transform(image)\n        image_id = self.data.index[index]\n        # if we don't have labels (e.g. for test set) just return the image and image id\n        if self.label is None:\n            sample = {\"image_id\": image_id, \"image\": image}\n        else:\n            label = torch.tensor(self.label.iloc[index].values, \n                                 dtype=torch.float)\n            sample = {\"image_id\": image_id, \"image\": image, \"label\": label}\n        return sample\n\n    def __len__(self):\n        return len(self.data)\n\nNotice that we are also defining a set of transformations, which are defined in the __init__ and called in the __getitem__ special methods. These are applied to each image before returning it. Here’s what each of those transformations do and why:\n\ntransforms.Resize((224, 224)) ResNet50 was trained on images of size 224x224 so we resize to the same dimensions here. See pytorch docs and the ResNet paper.\ntransforms.ToTensor() converts the image to a tensor. Since we are passing in a PIL Image at this point, PyTorch can recognize it as an RGB image and will automatically convert the input values which are in the range [0, 255] to a range of [0, 1]. See more from the PyTorch docs.\ntransforms.Normalize(...) normalizes the image tensors using the mean and standard deviation of ImageNet images. Because this transformation was applied to images when training the ResNet model, we want to do the same here with our images. See more from the PyTorch docs on pretrained models.\n\n\n\n\nNext, we need to load the dataset into a dataloader. The DataLoader class lets us iterate through our dataset in batches.\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataset = ImagesDataset(x_train, y_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32)\n\nThe data pieces are now largely in place!"
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html#training",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html#training",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "Now it’s time to start building our model and then training it.\nWe’ll use a pretrained ResNet50 model as our backbone. ResNets are one of the more popular networks for image classification tasks. The pretrained model outputs a 2048-dimension embedding, which we will then connect to two more dense layers, with a ReLU and Dropout step in between.\nThese final layers, defined in model.fc, are the new “head” of our model, and allow us to transform the image embeddings produced by the pretrained “backbone” into the 8-dimensional output required to learn the species classification task we’re tackling here. Prior to redefining it below, model.fc would be the final, dense layer connecting the 2048-dimension embedding to a 1000-dimension output (corresponding to the 1000 ImageNet classes that the pretrained model was trained on). We will instead prepare the model for the current task by redefining model.fc to produce an 8-dimensional output corresponding to our 8 species classes (including blanks).\nWe’ll also add a couple more layers in between. The ReLU layer introduces non-linearity into the model head, in effect activating important features and suppressing noise. And the Dropout layer is a commonly used regularization component that randomly drops some nodes from the previous layer’s outputs (10% of nodes in this case) during each training step, mitigating our risk of overfitting.\n\nfrom torch import nn\nimport torchvision.models as models\n\n\nmodel = models.resnet50(pretrained=True)\nmodel.fc = nn.Sequential(\n    nn.Linear(2048, 100),  # dense layer takes a 2048-dim input and outputs 100-dim\n    nn.ReLU(inplace=True),  # ReLU activation introduces non-linearity\n    nn.Dropout(0.1),  # common technique to mitigate overfitting\n    nn.Linear(\n        100, 8\n    ),  # final dense layer outputs 8-dim corresponding to our target classes\n)\n\n\n\nCross entropy loss (or log loss) is a commonly used loss function for multi-class (not multi-label) image classification. We’ll use this to compute loss for each training batch and then update our parameters accordingly.\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n\n\n\nWe’re now ready to train our model!\nWe’ll start simple and just run it for one epoch, but feel free to run it for more num_epochs if you’ve got the time. We hope to see a decreasing loss as training progresses, which will provide some evidence that the model is learning. Note that we haven’t frozen any weights in the pretrained model, a choice which you may want to revisit and we discuss in a little more detail below.\nFor each epoch we’ll iterate through the batches, and for each batch we’ll do the following: 1. Zero out the gradients. PyTorch will sum the gradients from past batches when doing its backward pass, so in order to make sure we are only using the gradients computed for the current batch, we zero out the gradients at the beginning of each batch. 2. Run the forward pass. 3. Compute the loss and track it. 4. Compute our gradients and update our weight parameters.\n\nnum_epochs = 1\n\ntracking_loss = {}\n\nfor epoch in range(1, num_epochs + 1):\n    print(f\"Starting epoch {epoch}\")\n\n    # iterate through the dataloader batches. tqdm keeps track of progress.\n    for batch_n, batch in tqdm(\n        enumerate(train_dataloader), total=len(train_dataloader)\n    ):\n\n        # 1) zero out the parameter gradients so that gradients from previous batches are not used in this step\n        optimizer.zero_grad()\n\n        # 2) run the foward step on this batch of images\n        outputs = model(batch[\"image\"])\n\n        # 3) compute the loss\n        loss = criterion(outputs, batch[\"label\"])\n        # let's keep track of the loss by epoch and batch\n        tracking_loss[(epoch, batch_n)] = float(loss)\n\n        # 4) compute our gradients\n        loss.backward()\n        # update our weights\n        optimizer.step()\n\nStarting epoch 1\n\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 194/194 [35:46&lt;00:00, 11.06s/it]\n\n\nNow let’s plot the loss by epoch and batch. The x-axis here is a tuple of (epoch, batch).\n\ntracking_loss = pd.Series(tracking_loss)\n\nplt.figure(figsize=(10, 5))\ntracking_loss.plot(alpha=0.2, label=\"loss\")\ntracking_loss.rolling(center=True, min_periods=1, window=10).mean().plot(\n    label=\"loss (moving avg)\"\n)\nplt.xlabel(\"(Epoch, Batch)\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=0)\n\n\n\n\n\n\n\n\nGood news, the loss is going down! This is an encouraging start, especially since we haven’t done anything fancy yet.\n\n\n\nWe have the model loaded in memory already, so we don’t really need to save the model, but it’s often useful to do this so we can use it again later.\nHere’s how:\n\ntorch.save(model, \"model.pth\")"
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html#evaluation",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html#evaluation",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "So far, not so bad. We’ve shown an improvement in the loss on the training set, but that tells us little about how our model will do on new data. Let’s reload our saved model and try generating some predictions on the evaluation split we created earlier.\n\nloaded_model = torch.load(\"model.pth\")\n\nWe create the eval dataset and dataloader just like we did earlier with the training dataset and dataloader:\n\neval_dataset = ImagesDataset(x_eval, y_eval)\neval_dataloader = DataLoader(eval_dataset, batch_size=32)\n\n\n\nWe’ll iterate through the eval dataloader in batches, just like we did for training, but this time we aren’t going to need to compute gradients or update weights. For each batch, we’ll do the following: 1. Run the forward pass to get the model output or logits 2. Apply a softmax function to convert the logits into probability space with range[0,1]. During training, the softmax operation was handled internally by nn.CrossEntropyLoss. We aren’t computing the loss now because we are just doing evaluation, but we still want the predictions to be in the range[0,1]. 3. Store the results in a dataframe for further analysis\n\npreds_collector = []\n\n# put the model in eval mode so we don't update any parameters\nmodel.eval()\n\n# we aren't updating our weights so no need to calculate gradients\nwith torch.no_grad():\n    for batch in tqdm(eval_dataloader, total=len(eval_dataloader)):\n        # 1) run the forward step\n        logits = model.forward(batch[\"image\"])\n        # 2) apply softmax so that model outputs are in range [0,1]\n        preds = nn.functional.softmax(logits, dim=1)\n        # 3) store this batch's predictions in df\n        # note that PyTorch Tensors need to first be detached from their computational graph before converting to numpy arrays\n        preds_df = pd.DataFrame(\n            preds.detach().numpy(),\n            index=batch[\"image_id\"],\n            columns=species_labels,\n        )\n        preds_collector.append(preds_df)\n\neval_preds_df = pd.concat(preds_collector)\neval_preds_df\n\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 65/65 [03:59&lt;00:00,  3.68s/it]\n\n\n\n\n\n\n\n\n\n\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\n\n\nZJ005376\n0.171700\n0.192484\n0.176443\n0.009301\n0.030189\n0.065699\n0.318819\n0.035365\n\n\nZJ011044\n0.001978\n0.002307\n0.001948\n0.000101\n0.001681\n0.990360\n0.000906\n0.000719\n\n\nZJ005242\n0.210068\n0.189871\n0.121386\n0.012617\n0.021859\n0.031385\n0.358011\n0.054803\n\n\nZJ004518\n0.238102\n0.253908\n0.109069\n0.008890\n0.023841\n0.037896\n0.283491\n0.044803\n\n\nZJ000101\n0.283641\n0.121174\n0.159294\n0.024840\n0.041801\n0.041539\n0.260422\n0.067289\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nZJ011868\n0.130162\n0.207689\n0.163008\n0.021061\n0.053528\n0.120070\n0.230920\n0.073561\n\n\nZJ002183\n0.247485\n0.180399\n0.125990\n0.009920\n0.018781\n0.022285\n0.352043\n0.043098\n\n\nZJ014186\n0.069278\n0.339674\n0.087901\n0.004488\n0.021389\n0.341253\n0.110601\n0.025416\n\n\nZJ011633\n0.223148\n0.143238\n0.156081\n0.032510\n0.046612\n0.049409\n0.277509\n0.071493\n\n\nZJ001374\n0.006084\n0.005903\n0.006707\n0.000377\n0.004165\n0.968445\n0.006105\n0.002214\n\n\n\n\n2061 rows × 8 columns\n\n\n\n\n\n\n\nFirst let’s review the species distribution we saw in the training set.\n\nprint(\"True labels (training):\")\ny_train.idxmax(axis=1).value_counts()\n\nTrue labels (training):\n\n\nmonkey_prosimian    973\nantelope_duiker     925\ncivet_genet         896\nblank               860\nleopard             841\nrodent              732\nbird                608\nhog                 348\ndtype: int64\n\n\nHere’s the distribution of our predictions on the eval set.\n\nprint(\"Predicted labels (eval):\")\neval_preds_df.idxmax(axis=1).value_counts()\n\nPredicted labels (eval):\n\n\nmonkey_prosimian    919\ncivet_genet         403\nleopard             329\nrodent              132\nblank               122\nantelope_duiker     106\nbird                 48\nhog                   2\ndtype: int64\n\n\nThe actual evaluation set is more evenly distributed than our predictions, so we already know there is some room for improvement here.\n\nprint(\"True labels (eval):\")\ny_eval.idxmax(axis=1).value_counts()\n\nTrue labels (eval):\n\n\nmonkey_prosimian    325\nantelope_duiker     308\ncivet_genet         298\nblank               287\nleopard             280\nrodent              244\nbird                203\nhog                 116\ndtype: int64\n\n\n\n\n\nNow let’s compute how accurate our model is and compare that against some trivial baseline models. First let’s get the labels with the highest score for each image.\n\neval_predictions = eval_preds_df.idxmax(axis=1)\neval_predictions.head()\n\nZJ005376    monkey_prosimian\nZJ011044             leopard\nZJ005242    monkey_prosimian\nZJ004518    monkey_prosimian\nZJ000101     antelope_duiker\ndtype: object\n\n\nRandom guessing across 8 classes would yield an accuracy of 12.5% (1/8). But we could construct a slightly better trivial model by always guessing the most common class (“monkey_prosimian” images in this case).\nIf we were to always guess that an image is monkey_prosimian, we could achieve accuracy of 15.8%.\n\neval_true = y_eval.idxmax(axis=1)\n\n(eval_true == \"monkey_prosimian\").sum() / len(eval_predictions)\n\n0.1576904415332363\n\n\nLet’s see how our model compares. We take the species with the highest score for each image (eval_predictions) and compare that to the true labels.\n\ncorrect = (eval_predictions == eval_true).sum()\naccuracy = correct / len(eval_predictions)\naccuracy\n\n0.49199417758369723\n\n\nOur accuracy on the evaluation set is about 50%, which is not a bad start for a very simple first pass and one epoch of training.\nLet’s look at the predictions from another angle.\nWe can see from the confusion matrix below that our model does reasonably well on some species, but we have plenty of room for improvement on antelopes, birds, hogs and blanks.\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfig, ax = plt.subplots(figsize=(10, 10))\ncm = ConfusionMatrixDisplay.from_predictions(\n    y_eval.idxmax(axis=1),\n    eval_preds_df.idxmax(axis=1),\n    ax=ax,\n    xticks_rotation=90,\n    colorbar=True,\n)\n\n\n\n\n\n\n\n\nThat’s where you come in! What can you do to improve on this benchmark?\nHere are some ideas you might want to try: * Train on the full training dataset. We’ve only used 50% of the training data so far. * Train for more epochs. We’ve only done 1 so far. * Try another pretrained model. For example, you may have more success with EfficientNet, or another ResNet model with more layers like ResNet152. See what’s available from pytorch here. You may also want to review which models are or have been state of the art for image classification tasks, for example on paperswithcode.com. Keep in mind that different models will require different input and output dimensions, so you’ll need to update how you construct model above. * Experiment with different loss functions. * Experiment with different learning rates or learning rate schedulers. * Add more layers to the model head (model.fc). * You also may want to consider freezing the weights in the backbone model and only training the head (model.fc). If this results in higher accuracy, that suggests the current approach may be overwriting the backbone weights in a problematic way. One approach here would be to train just the model head, and then unfreeze the backbone but train at a lower learning rate. * Training will be much faster using GPUs, but you will need to make some small tweaks to the code. * As you become more comfortable iterating through different versions of the model, you may want to try out PyTorch Lightning or Lightning Flash, which build upon PyTorch and eliminate a lot of boilerplate code, in addition to providing a more complete research framework for deep learning problems."
  },
  {
    "objectID": "posts/fastai-img-classif/data/animals/benchmark.html#create-submission",
    "href": "posts/fastai-img-classif/data/animals/benchmark.html#create-submission",
    "title": "Introduction to image classification using camera trap images",
    "section": "",
    "text": "Last but not least, we’ll want to participate in the competition and see where we stand on the leaderboard.\nTo do this we need to create predictions for the competition test set (not the eval set we used above). You don’t have labels for these.\nWe’ll create predictions in the same way we did for the eval set, but this time using the test_features we downloaded from the competition website.\n\ntest_dataset = ImagesDataset(test_features.filepath.to_frame())\ntest_dataloader = DataLoader(test_dataset, batch_size=32)\n\n\npreds_collector = []\n\n# put the model in eval mode so we don't update any parameters\nmodel.eval()\n\n# we aren't updating our weights so no need to calculate gradients\nwith torch.no_grad():\n    for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n        # run the forward step\n        logits = model.forward(batch[\"image\"])\n        # apply softmax so that model outputs are in range [0,1]\n        preds = nn.functional.softmax(logits, dim=1)\n        # store this batch's predictions in df\n        # note that PyTorch Tensors need to first be detached from their computational graph before converting to numpy arrays\n        preds_df = pd.DataFrame(\n            preds.detach().numpy(),\n            index=batch[\"image_id\"],\n            columns=species_labels,\n        )\n        preds_collector.append(preds_df)\n\nsubmission_df = pd.concat(preds_collector)\nsubmission_df\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140/140 [09:19&lt;00:00,  4.00s/it]\n\n\n\n\n\n\n\n\n\n\nantelope_duiker\nbird\nblank\ncivet_genet\nhog\nleopard\nmonkey_prosimian\nrodent\n\n\n\n\nZJ016488\n0.040287\n0.016456\n0.069033\n0.639093\n0.018142\n0.018959\n0.015982\n0.182047\n\n\nZJ016489\n0.257348\n0.117252\n0.138006\n0.024684\n0.049720\n0.057412\n0.294213\n0.061365\n\n\nZJ016490\n0.242068\n0.075779\n0.177219\n0.098522\n0.061314\n0.060584\n0.160722\n0.123793\n\n\nZJ016491\n0.009359\n0.010228\n0.009832\n0.000861\n0.008720\n0.952021\n0.005001\n0.003978\n\n\nZJ016492\n0.242449\n0.117502\n0.119074\n0.007304\n0.017310\n0.022663\n0.443947\n0.029752\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nZJ020947\n0.176772\n0.180421\n0.153451\n0.008706\n0.040353\n0.151517\n0.258835\n0.029944\n\n\nZJ020948\n0.270863\n0.149436\n0.135711\n0.009466\n0.028289\n0.054232\n0.311244\n0.040758\n\n\nZJ020949\n0.072901\n0.022102\n0.097448\n0.489528\n0.043097\n0.028081\n0.024680\n0.222163\n\n\nZJ020950\n0.167972\n0.235275\n0.123283\n0.008515\n0.028146\n0.047482\n0.340371\n0.048957\n\n\nZJ020951\n0.002522\n0.006116\n0.004133\n0.000110\n0.002457\n0.980648\n0.003158\n0.000856\n\n\n\n\n4464 rows × 8 columns\n\n\n\n\nLet’s check a couple things on submission_df before submitting to the platform. We’ll want to make sure our submission’s index and column labels match the submission format. (The DrivenData platform will do these data integrity checks as well, but it will be quicker to detect problems this way.)\n\nsubmission_format = pd.read_csv(\"submission_format.csv\", index_col=\"id\")\n\nassert all(submission_df.index == submission_format.index)\nassert all(submission_df.columns == submission_format.columns)\n\nLooks like we’re ready to submit! Save the dataframe out to a CSV file and then upload it via the Submissions page on the competition website.\n\nsubmission_df.to_csv(\"submission_df.csv\")\n\nHow did we do? We should get a score of ~1.8, though your results may differ slightly due to non-determinism in model training. (For reference, a randomly generated submission yields a score of something like ~2.4.)\n\n\n\nbenchmark submission\n\n\nNow it is up to you to improve on this benchmark!\nHead over to the competition for data and more background info, or the competition forum if you have any questions. Good luck!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Pandas Tests\n\n\nBackup of tests with the Pandas data manipulation tool.\n\n\n\nTests\n\n\n\n\n\n\nAfonso Matoso Magalhães\n\n\n\n\n\n\n\n\n\n\n\n\nfastai: Image Classifier\n\n\nApplication of the fastai library to multiple image classification datasets.\n\n\n\ndeep learning\n\n\nimage classification\n\n\n\n\n\n\nAfonso Matoso Magalhães\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to image classification using camera trap images\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]